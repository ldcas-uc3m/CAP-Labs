{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vas a ejecutar en la webapp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu noble-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu noble InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu noble-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu noble-backports InRelease\n",
      "Reading package lists... Done3m\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "netcat-openbsd is already the newest version (1.226-1ubuntu2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install netcat-openbsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9bcd4741f3f5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7ba4705ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "#spark.conf.set('spark.cores.max', '1')\n",
    "#spark.conf.set('spark.driver.memory','1000M')\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, importamos StreamingContext, que es el principal punto de entrada para toda la funcionalidad de streaming. Creamos un StreamingContext local con dos hilos de ejecución, y un intervalo de lote de 1 segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:22\n",
      "-------------------------------------------\n",
      "('hfjd', 1)\n",
      "('ksdjf', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:24\n",
      "-------------------------------------------\n",
      "('hfjd', 1)\n",
      "('ksdjf', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:26\n",
      "-------------------------------------------\n",
      "('hfjd', 1)\n",
      "('ksdjf', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:28\n",
      "-------------------------------------------\n",
      "('hfjd', 1)\n",
      "('ksdjf', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:30\n",
      "-------------------------------------------\n",
      "('hfjd', 1)\n",
      "('ksdjf', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:32\n",
      "-------------------------------------------\n",
      "('hfjd', 0)\n",
      "('ksdjf', 0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:34\n",
      "-------------------------------------------\n",
      "('hfjd', 0)\n",
      "('ksdjf', 0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-10-16 16:27:36\n",
      "-------------------------------------------\n",
      "('hfjd', 0)\n",
      "('ksdjf', 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 2)\n",
    "sc.setCheckpointDir(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando este contexto, podemos crear un DStream que represente los datos de streaming de una fuente TCP, especificados como nombre de host (por ejemplo, localhost) y puerto (por ejemplo, 1234).\n",
    "\n",
    "Desde la máquina (`docker exec -it spark-lab-spark-1 /bin/bash`)\n",
    "```bash\n",
    "nc -l 127.0.0.1 1234\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"127.0.0.1\", 1235)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DStream línea representa el flujo de datos que se recibirá del servidor de datos. Cada registro en este DStream es una línea de texto. A continuación, queremos dividir las líneas por el espacio en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap es una operación de uno a muchos DStream que crea un nuevo DStream generando múltiples registros nuevos de cada registro en el DStream de origen. En este caso, cada línea se dividirá en múltiples palabras y el flujo de palabras se representa como las palabras DStream. A continuación, queremos contar estas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 10)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()\n",
    "wordCounts.saveAsTextFiles(\"output/spark_streaming/streaming_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras DStream son mapeadas más adelante (transformación uno a uno) a un DStream de (palabra, 1) pares, que luego se reduce para obtener la frecuencia de palabras en cada lote de datos. Finalmente, wordCounts.pprint() imprimirá algunos de los recuentos generados cada segundo.\n",
    "\n",
    "Nótese que cuando estas líneas se ejecutan, Spark Streaming sólo establece el cómputo que realizará cuando se inicie, y aún no se ha iniciado un verdadero procesamiento. Para iniciar el procesamiento después de que todas las transformaciones han sido configuradas, finalmente llamamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
